<!DOCTYPE html>
    <html lang="es">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>El dilema de Anthropic: innovación acelerada y riesgos éticos en la carrera por la IA.</title>
        <style>
            body {
                max-width: 800px;
                margin: auto;
                padding: 5%;
            }
            .btnL {
                background: #d1d1d1;
                border-radius: 7px;
                padding: 18px;
                min-width: 100%;
                color: #000000;
                font-size: medium;
                min-height: 70px;
            }
            button {
                background-color: #d1d1d1;
                color: white;
                padding: 10px 20px;
                border: none;
                border-radius: 4px;
                cursor: pointer;
                margin: 5px;
            }
            button:disabled {
                background-color: #d1d1d1;
                cursor: not-available;
            }
            button:hover:not(:disabled) {
                background-color: #d1d1d1;
            }
            #voiceSelect {
                margin: 10px 0;
                padding: 5px;
            }
            .controls {
                margin: 10px 0;
            }

            #fullscreenButton {
                position: fixed;
                top: 20px;
                right: 20px;
                padding: 10px;
                background-color: transparent;
                border: none;
                cursor: pointer;
                border-radius: 5px;
            }

            #fullscreenIcon {
                width: 24px;
                height: 24px;
            }

        </style>
    </head>
    <body>
        <button id="fullscreenButton">
        <svg id="fullscreenIcon" viewBox="0 0 24 24" fill="currentColor">
            <path d="M5 5h5v5H5zM14 5h5v5h-5zM5 14h5v5H5zM14 14h5v5h-5z"/>
        </svg>
        </button>

        <div id="texto">
            <h1>El dilema de Anthropic: innovación acelerada y riesgos éticos en la carrera por la IA.</h1>
            <p>En marzo, Dario Amodei reunió a su equipo en Anthropic para abordar un tema que inquietaba a muchos dentro de la empresa: la posibilidad de que la inteligencia artificial (IA) que estaban desarrollando pudiera automatizar la mayoría de las tareas técnicas, como la escritura y depuración de código, en un futuro muy cercano.</p><p>Según consignó Bloomberg, Amodei advirtió a sus empleados que la tecnología de Anthropic podría transformar radicalmente la forma en que la compañía organiza su trabajo, hasta el punto de ralentizar la contratación para evitar despidos derivados de la automatización. Esta preocupación interna refleja el dilema más amplio que enfrenta la industria de la IA: cómo avanzar rápidamente en el desarrollo de sistemas cada vez más potentes sin sacrificar la seguridad ni el bienestar de quienes los construyen.</p><p>De acuerdo con Bloomberg, la tensión entre innovación y responsabilidad se hizo especialmente evidente en febrero, cuando miembros del equipo de seguridad de Anthropic alertaron a Amodei sobre el riesgo de que el modelo Claude 3.7 Sonnet pudiera ser utilizado para crear armas biológicas. En ese momento, la empresa se encontraba bajo una fuerte presión competitiva, ya que otras compañías del sector también se apresuraban a presentar sus propios modelos. Además, Amodei estaba cerrando una ronda de inversión multimillonaria que valoraba a Anthropic en más de 61.000 millones de dólares. A pesar de la urgencia, Amodei decidió priorizar la seguridad y ordenó a su equipo que se tomara el tiempo necesario para realizar pruebas rigurosas, retrasando el lanzamiento del modelo.</p><p>Anthropic había anticipado situaciones de este tipo y, según detalló Bloomberg, implementó una política interna denominada Responsible Scaling Policy, inspirada en los estándares de bioseguridad de laboratorios estadounidenses. Esta política establece diferentes niveles de seguridad para los modelos de IA, denominados AI Safety Levels (ASL).</p><p>Los modelos clasificados como ASL-2 pueden ofrecer instrucciones sobre la creación de armas biológicas, pero no de manera fiable ni con más detalle que una búsqueda en internet. Un modelo ASL-3 podría facilitar significativamente la creación o despliegue de tales armas a usuarios con conocimientos técnicos básicos. Hasta la fecha, los modelos lanzados por Anthropic se mantenían en ASL-2 o inferior, pero la empresa prevé alcanzar ASL-3 en breve y ya ha comenzado a reforzar sus salvaguardas.</p><p>Bloomberg reportó que, en caso de que un modelo alcance ASL-3, las directrices internas de Anthropic exigen la implementación de medidas adicionales, como fortalecer la protección del código y bloquear respuestas peligrosas. Si estas medidas no pueden aplicarse de inmediato, la empresa debe recurrir a soluciones temporales, como debilitar intencionadamente el modelo o posponer su lanzamiento. Tras casi una semana de pruebas, el equipo concluyó que Claude 3.7 Sonnet no era tan potente como se temía, por lo que se lanzó con un leve retraso y, hasta el momento, sin consecuencias graves.</p><p>La decisión de retrasar el lanzamiento resultó “dolorosa” para Amodei, según sus propias palabras recogidas por la agencia norteamericana, debido a la presión del mercado. Sin embargo, el CEO de Anthropic sostiene que la responsabilidad moral de la empresa exige comunicar de manera clara y frecuente los riesgos asociados a la IA. Amodei está convencido de que la IA transformará el mundo, creando lo que describe como un “país de genios en un centro de datos”. Esta tecnología, según él, podría curar el cáncer, pero también provocar la pérdida de empleos para la mayoría de la población mundial. Amodei estima que este cambio social masivo podría ocurrir tan pronto como el próximo año, y casi con certeza antes de 2030.</p><p>Anthropic fue fundada hace cuatro años con el objetivo de liderar esta transformación de manera responsable. No obstante, la empresa también enfrenta la realidad de un mercado en el que los clientes están dispuestos a pagar sumas considerables por acceder a su tecnología. Según datos de abril citados por Bloomberg, la compañía proyectaba ingresos anuales de 2.000 millones de dólares, el doble que cuatro meses antes. A pesar de este crecimiento, Anthropic aún no es rentable debido a los elevados costos de entrenamiento de sus sistemas de IA, que podrían alcanzar los 100.000 millones de dólares para los modelos más avanzados.</p><p>La demanda de IA más potente no deja de crecer, y Anthropic espera alcanzar pronto el nivel ASL-3. Para prepararse, la empresa ha contratado a varios investigadores destacados provenientes de OpenAI, algunos de los cuales han criticado a su antiguo empleador por alejarse de sus compromisos originales con la seguridad. Según Bloomberg, estos nuevos fichajes no dudarían en alzar la voz si Anthropic incumple sus propias promesas en materia de seguridad.</p><p>El ritmo de desarrollo de la IA genera debate tanto dentro como fuera de la empresa. Al igual que Google, Meta Platforms Inc. y OpenAI, Anthropic ha experimentado retrasos en la publicación de nuevas versiones de sus modelos más avanzados. Algunos escépticos, citados por Bloomberg, sugieren que el énfasis en los peligros de la IA podría estar destinado a magnificar la percepción de su poder real. Por otro lado, quienes se preocupan por la seguridad advierten que la presión del mercado podría llevar a las empresas a tomar decisiones irresponsables, ya que los inversores que han aportado 14.000 millones de dólares a Anthropic esperan que la compañía supere a competidores como OpenAI, DeepSeek o Meta.</p><p>Amodei reconoce que resulta difícil oponerse frontalmente a las fuerzas del mercado, pero aspira a crear una “carrera hacia la cima”, en la que Anthropic demuestre que es posible desarrollar IA transformadora sin poner en riesgo a la humanidad. Amodei, originario de San Francisco, nunca se consideró parte del mundo tecnológico. Creció en el Mission District antes de que la llegada de capital tecnológico transformara el barrio. Su familia era de clase trabajadora: su padre, huérfano en Italia, fue artesano del cuero hasta que problemas de salud le obligaron a dejar el oficio; su madre trabajaba como gestora de proyectos en una biblioteca.</p><p>La hermana de Amodei, Daniela Amodei, también cofundadora y presidenta de Anthropic, recuerda que Dario mostró desde pequeño un talento inusual para las matemáticas y las ciencias. De niño, pasaba días enteros contando números y, en la adolescencia, asistió a clases en la Universidad de California en Berkeley antes de estudiar física en el California Institute of Technology y luego en la Universidad de Stanford. Su interés por la IA surgió tras leer “The Singularity Is Near” de Ray Kurzweil, que pronosticaba que la IA alcanzaría la inteligencia humana en 2029.</p><p>Tras obtener su licenciatura en 2006, Amodei se orientó hacia aplicaciones neurológicas y biológicas de la física, realizando su doctorado en biofísica en Princeton University. Su investigación se centró en las estructuras neuronales de las células ganglionares de anfibios, lo que le llevó a diseccionar salamandras para examinar sus retinas. Aunque no le entusiasmaban las implicaciones éticas de estos experimentos, su motivación era resolver problemas de biología y salud humana.</p><p>Lo que realmente le desanimó fue la lentitud del trabajo científico en comparación con los avances que observaba en el desarrollo de redes neuronales artificiales. En 2014, Andrew Ng lo reclutó para trabajar en IA en una unidad de Baidu Inc.. Tras un año allí y otro en Google Brain, Amodei comenzó a reflexionar sobre las implicaciones éticas del rápido progreso de la IA. En 2016, publicó un artículo influyente sobre los problemas concretos de seguridad en IA, identificando cinco áreas clave donde la tecnología podría tener efectos no deseados y perjudiciales.</p><p>Aunque Google era el lugar ideal para un investigador de IA en esa época, Amodei se sintió más alineado con la misión de OpenAI, un laboratorio sin fines de lucro que buscaba desarrollar IA de manera segura. Se unió en 2016 como líder de investigación en seguridad y compartió vivienda en San Francisco con varios futuros cofundadores de Anthropic, incluida su hermana Daniela. En ese entorno, ambos se relacionaron con círculos vinculados al altruismo efectivo, una filosofía que prioriza el pensamiento racional para mejorar el mundo, popular entre quienes se interesan por la seguridad en IA.</p><p>La contribución más destacada de Amodei en OpenAI fue el desarrollo del concepto de “leyes de escalado”, que sostiene que se pueden lograr mejoras fundamentales en una red neuronal aumentando el tamaño del modelo y añadiendo más datos y capacidad de cómputo. Esta idea, que desafió la creencia tradicional de que los avances dependían principalmente de mejores algoritmos, fue clave en el auge de los grandes modelos de lenguaje que dominan la IA actual.</p><p>Con el tiempo, Amodei perdió confianza en la dirección de OpenAI y, en 2020, junto a seis colegas, fundó Anthropic con la promesa de crear un laboratorio de IA más responsable. Según Bloomberg, la salida de estos empleados generó gran interés en Silicon Valley, donde las historias de rivalidad entre startups son habituales. Amodei ha explicado que no hubo un punto de quiebre específico, sino una acumulación de razones para buscar una forma diferente de operar, basada en la confianza mutua.</p><p>En sus inicios, Anthropic enfrentó dudas sobre su capacidad para competir con OpenAI, que contaba con más capital y ventaja en el desarrollo de modelos. Eric Schmidt, ex CEO de Google y uno de los primeros inversores en Anthropic, recuerda que muchos pensaban que la empresa no podría escalar por falta de financiación. Sin embargo, Anthropic ha logrado posicionarse como un competidor serio, con tecnología comparable y una base creciente de clientes en sectores como finanzas, farmacéutica y desarrollo de software. Aunque ofrece un chatbot de IA llamado Claude, la empresa se enfoca menos en el mercado de consumo que OpenAI.</p><p>Schmidt relató a Bloomberg que, en una visita a Amodei y su pareja en 2018, quedó impresionado por su determinación de establecer Anthropic como una public benefit corporation, una organización con fines de lucro dedicada a una misión pública. Aunque Schmidt sugirió que optara por una estructura tradicional de startup, Amodei se negó. En los primeros días de la empresa, hubo debates sobre si debía financiarse filantrópicamente o centrarse exclusivamente en la investigación de seguridad, pero finalmente optaron por mantener abiertas sus opciones.</p><p>Hemant Taneja, CEO de General Catalyst, firma que respaldó a Anthropic en su última ronda de financiación, considera que la empresa tiene el potencial de ser una de las ganadoras en la actual ola de IA, aunque reconoce la incertidumbre asociada a una inversión de tal magnitud.</p><p>Mientras avanza, Anthropic ha cultivado una reputación de tomarse en serio la seguridad y la responsabilidad, en contraste con la empresa de la que surgió. La breve destitución de Sam Altman en OpenAI en 2023, seguida de cuestionamientos sobre la integridad y el compromiso de la compañía con su misión original, ha reforzado esta percepción. Aunque Amodei evita criticar abiertamente a su antiguo empleador, Anthropic ha lanzado campañas publicitarias en San Francisco con lemas como “IA en la que puedes confiar” y “La que no tiene todo el drama”. A diferencia de otros ejecutivos tecnológicos, Amodei no ha intentado congraciarse con la administración de Donald Trump, manteniendo el mismo mensaje que durante la presidencia de Joe Biden.</p><p>En enero, Amodei asistió por primera vez al Foro Económico Mundial en Davos, donde cerró un contrato multianual con AIG para analizar datos de clientes en el proceso de suscripción de seguros. Peter Zaffino, CEO de AIG, eligió a Anthropic por su enfoque en la confiabilidad y precisión en la citación de fuentes de datos, aspectos cruciales en la industria aseguradora. Zaffino destacó la rapidez con la que Amodei comprendió los objetivos empresariales, afirmando: “Por lo que Dario carece de experiencia empresarial, el algoritmo en su cerebro se mueve muy rápido”.</p><p>A pesar de su reciente fortuna, Amodei mantiene un estilo de vida sencillo, viviendo en una casa alquilada en un suburbio al sur de San Francisco y criando gallinas en su jardín. Ha prometido donar “la gran mayoría” de su riqueza a causas benéficas. En la sede de Anthropic, Amodei prefiere vestir ropa cómoda, lo que, según él, le ayuda a pensar mejor.</p><p>La cultura de Anthropic refleja el enfoque académico de Amodei. Cada dos semanas, los empleados —a quienes llaman “Ants”— asisten a charlas de una hora conocidas como “Dario vision quests”, acompañadas de documentos que deben leer previamente. Bajo su liderazgo, la empresa investiga temas que no generan ingresos inmediatos, como la interpretabilidad mecanicista (el estudio de cómo los algoritmos toman decisiones) y el bienestar de la IA (la ética de interactuar con computadoras si algún día alcanzan la consciencia).</p><p>Bloomberg señala que, gracias a este historial, Anthropic ha consolidado su imagen como una empresa comprometida con el desarrollo responsable de la IA, en contraste con otras compañías tecnológicas que parecen limitarse a declaraciones superficiales o incluso rechazan la idea de un marco ético si este ralentiza el progreso. Matthew Yglesias, escritor especializado en economía y políticas públicas, opinó: “Dario y todo el equipo merecen crédito y confianza por actuar de buena fe en materia de seguridad. Pero no está claro si eso cambia la situación estructural. Si estás compitiendo, es difícil ser seguro, incluso si actúas de buena fe”.</p><p>En cuanto a aplicaciones concretas, Anthropic ha destacado en el desarrollo de herramientas para programadores. Recientemente lanzó Claude Code, una aplicación para codificadores, y su tecnología impulsa otras aplicaciones populares como Cursor. Según el informe económico de febrero de la empresa, el 37% de las interacciones laborales con Claude estaban relacionadas con la programación, la categoría más alta, seguida por artes y medios con un 10%. Amodei afirmó que la automatización de la codificación ha sido el área de mayor crecimiento en los últimos meses.</p><p>El uso de IA para programar no genera el mismo debate emocional que la creación de música o arte, y la mayoría de los programadores han aceptado la integración de estas herramientas en su trabajo. Una encuesta de GitHub Inc. a 2.000 empleados técnicos reveló que el 97% había utilizado herramientas de codificación asistida por IA. Sin embargo, la posibilidad de pérdida de empleos en este campo resulta más tangible que los riesgos de seguridad, como la creación de armas. Anthropic descubrió que el 79% de los programadores que usan Claude Code lo hacen para automatizar tareas, no solo para complementarlas.</p><p>Amodei abordó este tema en una charla en Washington, DC, organizada por el Council on Foreign Relations, donde predijo que la IA podría escribir casi todo el código informático en un año. Sus declaraciones generaron temor y escepticismo, aunque aclaró que los humanos seguirán participando en el proceso, definiendo qué tipo de aplicaciones crear y cómo integrarlas. “Al final del día, estos modelos serán mejores que todos nosotros en todo. Tenemos que afrontarlo. A nivel social, todos debemos afrontarlo. Mi objetivo no es crear una subclase en el periodo previo a eso”, afirmó Amodei.</p><p>El impacto de la automatización también afecta a los propios empleados de Anthropic. Jack Clark, cofundador y jefe de políticas, comentó que el ambiente en la empresa refleja la inminencia de estos cambios. Amodei comunicó a su equipo que la empresa podría reducir la contratación debido a Claude, pero que evitaría despidos y ayudaría a los programadores a adaptarse a sus nuevos roles.</p><p>En un memorando interno, Amodei estimó que existía un 70% de probabilidad de que, en algún momento de este año, la IA pase de ser una herramienta útil a convertirse en algo indispensable que realice la mayoría de las tareas técnicas, duplicando la velocidad de ejecución de Anthropic. “La mayor parte del progreso en IA provendrá de la propia IA”, escribió, aunque matizó que los humanos seguirán desempeñando un papel central “probablemente durante un tiempo, debido a la ventaja comparativa”. No obstante, advirtió que el papel humano podría reducirse gradualmente hasta que la IA comience a crear nuevas IA en un ciclo recursivo.</p><p>Si se alcanza esta capacidad, los modelos de Anthropic escalarían rápidamente en su sistema de niveles de seguridad. Un modelo ASL-4 podría automatizar completamente el trabajo de un investigador principiante y remoto en la empresa, mientras que un ASL-5 tendría la capacidad de mejorarse a sí mismo de forma acelerada.</p><p>En su ensayo “Machines of Loving Grace”, que primero compartió internamente y luego publicó en su blog personal en octubre, Amodei describió el escenario ideal si todo sale bien con la IA. Basándose en su experiencia en biología, predijo que la IA acelerará los descubrimientos científicos diez veces, ayudando a curar casi todas las enfermedades infecciosas, la mayoría de los cánceres y el Alzheimer, y duplicando la esperanza de vida humana. Anthropic distribuye copias de bolsillo de este ensayo a sus empleados.</p><p>No obstante, el tono cambia al abordar la relación entre la IA, el trabajo y el sentido de la vida. Amodei reconoce que este aspecto es “más difuso y difícil de predecir”. Anticipa que la IA podría reemplazar la mayor parte del trabajo humano, obligando a las personas a depender de una renta básica universal u otros mecanismos de redistribución, salvo que encuentren nuevas formas de aportar valor económico. “Probablemente tendremos que luchar para lograr un buen resultado aquí: las direcciones explotadoras o distópicas son claramente posibles y deben evitarse”, escribió Amodei, dejando abierta la puerta a futuras reflexiones sobre estos desafíos.</p><p>Temas Relacionados</p><p></p>
        </div>
        <br><br><br>
            <div style="display: none;">
                <label for="voiceSelect">Seleccionar voz:</label>
                <select id="voiceSelect"></select>
            </div>
            <div class="controls">
                <button onclick="startReading()" id="playButton">
                    🔊 Leer
                </button>
                <button onclick="pauseReading()" id="pauseButton" disabled style="display: none;">
                    ⏸️ Pausar
                </button>
                <button onclick="resumeReading()" id="resumeButton" disabled style="display: none;">
                    ▶️ Continuar
                </button>
                <button onclick="stopReading()" id="stopButton" disabled>
                    ⏹️ Detener
                </button>
            </div>
            <div id="progress" style="display: none;"></div>
        <br><br><br>
        <button class="btnL" onclick="compartir()">Compartir</button>
        <br><br><br>
        <button class="btnL" onclick="window.location.href='https://www.infobae.com/tecno/2025/06/03/el-dilema-de-anthropic-innovacion-acelerada-y-riesgos-eticos-en-la-carrera-por-la-ia/'">Ver en web original</button>
        <br><br><br>
        <!-- <button class="btnL" onclick="window.location.href='https://matenews.github.io/MateNews/'">Volver a la página principal</button>
        <br><br><br> -->


        <script>
            function compartir() {
            if (navigator.share) {
                navigator.share({
                title: 'El dilema de Anthropic: innovación acelerada y riesgos éticos en la carrera por la IA.', // Opcional, pero recomendado     
                text: 'El dilema de Anthropic: innovación acelerada y riesgos éticos en la carrera por la IA.',  // Opcional
                url: 'https://www.infobae.com/tecno/2025/06/03/el-dilema-de-anthropic-innovacion-acelerada-y-riesgos-eticos-en-la-carrera-por-la-ia/'
                })
                .then(() => console.log('Contenido compartido exitosamente'))
                .catch((error) => console.error('Error al compartir:', error));
            } else {
                // Código alternativo si la API Web Share no está disponible
                console.log("Web Share API no soportada");
                // Puedes usar aquí la opción 2 (botones personalizados con JavaScript) de mi respuesta anterior
            }
            }
        </script>


        <script>
            let speechSynthesis = window.speechSynthesis;
            let currentUtterance = null;
            let isPaused = false;
            let isReading = false;
            let textBlocks = [];
            let currentBlockIndex = 0;
            let lastSpokenIndex = 0;
            const BLOCK_SIZE = 800;

            function updateButtons(speaking) {
                document.getElementById('playButton').disabled = speaking;
                document.getElementById('pauseButton').disabled = !speaking;
                document.getElementById('resumeButton').disabled = !isPaused;
                document.getElementById('stopButton').disabled = !speaking;
            }

            function updateProgress() {
                const progressDiv = document.getElementById('progress');
                if (isReading) {
                    progressDiv.textContent = `Leyendo bloque ${currentBlockIndex + 1} de ${textBlocks.length}`;
                } else {
                    progressDiv.textContent = '';
                }
            }

            function splitIntoBlocks(text) {
                // Dividir el texto en oraciones usando puntuación común
                const sentences = text.replace(/([.!?])\s+/g, "$1|").split("|");

                let blocks = [];
                let currentBlock = '';

                sentences.forEach(sentence => {
                    // Si la oración es muy larga, dividirla en la coma más cercana al BLOCK_SIZE
                    if (sentence.length > BLOCK_SIZE) {
                        let subSentences = sentence.replace(/,\s+/g, ",|").split("|");
                        subSentences.forEach(subSentence => {
                            if (currentBlock.length + subSentence.length > BLOCK_SIZE) {
                                if (currentBlock) blocks.push(currentBlock.trim());
                                currentBlock = subSentence;
                            } else {
                                currentBlock += (currentBlock ? ' ' : '') + subSentence;
                            }
                        });
                    } else {
                        if (currentBlock.length + sentence.length > BLOCK_SIZE) {
                            blocks.push(currentBlock.trim());
                            currentBlock = sentence;
                        } else {
                            currentBlock += (currentBlock ? ' ' : '') + sentence;
                        }
                    }
                });

                if (currentBlock) blocks.push(currentBlock.trim());
                return blocks;
            }

            function speakBlock(index) {
                if (index >= textBlocks.length) {
                    isReading = false;
                    updateButtons(false);
                    updateProgress();
                    return;
                }

                currentUtterance = new SpeechSynthesisUtterance(textBlocks[index]);
                currentUtterance.rate = parseFloat(1.3);
                lastSpokenIndex = index;


                currentUtterance.onend = function() {
                    if (isReading && !isPaused) {
                        currentBlockIndex++;
                        updateProgress();
                        speakBlock(currentBlockIndex);
                    }
                };

                currentUtterance.onerror = function(event) {
                    console.error('Error en la lectura:', event);
                    isReading = false;
                    updateButtons(false);
                    updateProgress();
                };

                speechSynthesis.speak(currentUtterance);
            }

            function startReading() {
                const text = document.getElementById('texto').innerText;
                if (!text) return;

                // Detener cualquier lectura previa
                stop();

                // Preparar nueva lectura
                textBlocks = splitIntoBlocks(text);
                currentBlockIndex = 0;
                isReading = true;
                isPaused = false;

                updateButtons(true);
                updateProgress();
                speakBlock(currentBlockIndex);
            }

            function pause() {
                speechSynthesis.pause();
                isPaused = true;
                updateButtons(true);
            }

            function resume() {
                speechSynthesis.resume();
                isPaused = false;
                updateButtons(true);
            }

            function stop() {
                speechSynthesis.cancel();
                isReading = false;
                isPaused = false;
                currentBlockIndex = 0;
                updateButtons(false);
                updateProgress();
            }

            function pauseReading() {
                if (!isReading) return;

                speechSynthesis.pause();
                isPaused = true;
                isReading = false;
                updateButtons(true);
                updateProgress();
            }

            function resumeReading() {
                if (!isPaused) return;

                isPaused = false;
                isReading = true;

                // Si el utterance actual ya terminó, comenzar con el siguiente bloque
                if (lastSpokenIndex === currentBlockIndex) {
                    currentBlockIndex++;
                }

                updateButtons(true);
                updateProgress();

                // Si hay un utterance pausado, resumirlo
                if (speechSynthesis.paused) {
                    speechSynthesis.resume();
                } else {
                    // Si no hay utterance pausado, comenzar desde el bloque actual
                    speakBlock(currentBlockIndex);
                }
            }

            function stopReading() {
                speechSynthesis.cancel();
                finishReading();
            }

            function finishReading() {
                isReading = false;
                isPaused = false;
                currentBlockIndex = 0;
                lastSpokenIndex = -1;
                currentUtterance = null;
                updateButtons(false);
                updateProgress();
            }

        </script>        

        <script>

            var elem = document.documentElement;
            var fullscreenButton = document.getElementById("fullscreenButton");
            var fullscreenIcon = document.getElementById("fullscreenIcon");

            fullscreenButton.addEventListener("click", function() {
            if (document.fullscreenElement || document.mozFullScreenElement || document.webkitFullscreenElement || document.msFullscreenElement) {
                if (document.exitFullscreen) {
                document.exitFullscreen();
                } else if (document.mozCancelFullScreen) { /* Firefox */
                document.mozCancelFullScreen();
                } else if (document.webkitExitFullscreen) { /* Chrome, Safari and Opera */
                document.webkitExitFullscreen();
                } else if (document.msExitFullscreen) { /* IE/Edge */
                document.msExitFullscreen();
                }
            } else {
                if (elem.requestFullscreen) {
                elem.requestFullscreen();
                } else if (elem.mozRequestFullScreen) { /* Firefox */
                elem.mozRequestFullScreen();
                } else if (elem.webkitRequestFullscreen) { /* Chrome, Safari and Opera */
                elem.webkitRequestFullscreen();
                } else if (elem.msRequestFullscreen) { /* IE/Edge */
                elem.msRequestFullscreen();
                }
            }
            });

            document.addEventListener('fullscreenchange', (event) => {
            if (!document.fullscreenElement) {
                fullscreenIcon.innerHTML = '<path d="M5 5h5v5H5zM14 5h5v5h-5zM5 14h5v5H5zM14 14h5v5h-5z"/>';
            } else {
                fullscreenIcon.innerHTML = '<path d="M7 14H5v5h5v-2H7v-3zm-2-4h2V7h3V5H5v5zm12 7h-3v2h5v-5h-2v3zM14 5v2h3v3h2V5h-5z"/>';
            }
            });
        </script>

    </body>
    </html>